
import os
import json
import joblib
import numpy as np
from pathlib import Path

# å®šä¹‰è·¯å¾„
BASE_DIR = Path(__file__).resolve().parent.parent.parent  # media-backend
# media-backend -> code -> media -> lab -> data
DATA_ROOT = BASE_DIR.parent.parent.parent / "data"               # lab/data
PCA_MODEL_PATH = BASE_DIR / "app/data/pca_model.pkl"
OUTPUT_PATH = BASE_DIR / "app/data/analysis_dataset.json"

def load_pca():
    if not PCA_MODEL_PATH.exists():
        print(f"âŒ PCA model not found at {PCA_MODEL_PATH}")
        return None
    print(f"âœ… Loading PCA model from {PCA_MODEL_PATH}...")
    return joblib.load(PCA_MODEL_PATH)

def safe_float(v):
    try:
        return float(v)
    except:
        return 0.0

def process_exp1(pca):
    """å¤„ç†å®éªŒ1æ•°æ®"""
    path = DATA_ROOT / "1/experiment_embeddings.json"
    if not path.exists():
        print(f"âš ï¸ Exp1 file not found: {path}")
        return []
        
    print(f"Processing Exp1: {path}")
    points = []
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
        for group_id, items in data.get("experiment_data", {}).items():
            # åŸå§‹å¥
            if "original_embedding" in items:
                emb = np.array(items["original_embedding"]).reshape(1, -1)
                xy = pca.transform(emb)[0]
                points.append({
                    "dataset": "Exp1",
                    "id": f"{group_id}_orig",
                    "text": items.get("original_sentence", "")[:50],
                    "x": safe_float(xy[0]),
                    "y": safe_float(xy[1]),
                    "type": "human_orig"
                })
            
            # ä¿®æ”¹è½®æ¬¡
            for round_idx, r in enumerate(items.get("modification_rounds", [])):
                # éšæœºä¿®æ”¹
                if "random_mod_embedding" in r:
                    emb = np.array(r["random_mod_embedding"]).reshape(1, -1)
                    xy = pca.transform(emb)[0]
                    points.append({
                        "dataset": "Exp1",
                        "id": f"{group_id}_r{round_idx}_rand",
                        "text": r.get("random_modification", "")[:50],
                        "x": safe_float(xy[0]),
                        "y": safe_float(xy[1]),
                        "type": "ai_noise"
                    })
                
                # ä¼˜åŒ–ç»“æœ
                if "optimized_embedding" in r:
                    emb = np.array(r["optimized_embedding"]).reshape(1, -1)
                    xy = pca.transform(emb)[0]
                    points.append({
                        "dataset": "Exp1",
                        "id": f"{group_id}_r{round_idx}_opt",
                        "text": r.get("optimized_result", "")[:50],
                        "x": safe_float(xy[0]),
                        "y": safe_float(xy[1]),
                        "type": "human_opt"
                    })
    except Exception as e:
        print(f"âŒ Error processing Exp1: {e}")
        
    return points

def process_exp2(pca):
    """å¤„ç†å®éªŒ2æ•°æ®"""
    path = DATA_ROOT / "2/experiment_2a_data.json"
    if not path.exists():
        print(f"âš ï¸ Exp2 file not found: {path}")
        return []

    print(f"Processing Exp2: {path}")
    points = []
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

        for key, items in data.get("experiment_data", {}).items():
            # è¿™é‡Œ experiment_data çš„ç»“æ„å¯èƒ½æ˜¯ä¸€å±‚
            # åŒæ ·æ‰¾ embedding
            if "original_embedding" in items:
                emb = np.array(items["original_embedding"]).reshape(1, -1)
                xy = pca.transform(emb)[0]
                points.append({
                    "dataset": "Exp2",
                    "id": f"{key}_orig",
                    "text": items.get("original_sentence", "")[:50],
                    "x": safe_float(xy[0]),
                    "y": safe_float(xy[1]),
                    "type": "human_orig"
                })

            for round_idx, r in enumerate(items.get("modification_rounds", [])):
                if "selected_embedding" in r:
                    emb = np.array(r["selected_embedding"]).reshape(1, -1)
                    xy = pca.transform(emb)[0]
                    points.append({
                        "dataset": "Exp2",
                        "id": f"{key}_r{round_idx}_sel",
                        "text": r.get("selected_result", "")[:50],
                        "x": safe_float(xy[0]),
                        "y": safe_float(xy[1]),
                        "type": "human_select"
                    })
    except Exception as e:
        print(f"âŒ Error processing Exp2: {e}")
        
    return points

def process_exp4(pca):
    """å¤„ç†å®éªŒ4æ•°æ®"""
    # å®éªŒ4æœ‰ä¸¤ä¸ª json: experiment_4a_data.json å’Œ experiment_4b_data(AI).json
    files = ["4/experiment_4a_data.json", "4/experiment_4b_data(AI).json"]
    points = []
    
    for fname in files:
        path = DATA_ROOT / fname
        if not path.exists():
            continue
            
        print(f"Processing Exp4: {path}")
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                
            # Exp4 ç»“æ„å·®å¼‚: root keyå¯èƒ½æ˜¯ "data" æˆ– "experiment_data"
            content = data.get("data") or data.get("experiment_data", {})
            
            for key, items in content.items():
                if "original_embedding" in items:
                    emb = np.array(items["original_embedding"]).reshape(1, -1)
                    xy = pca.transform(emb)[0]
                    points.append({
                        "dataset": "Exp4",
                        "id": f"{key}_orig",
                        "text": items.get("original_sentence") or items.get("original", "")[:50],
                        "x": safe_float(xy[0]),
                        "y": safe_float(xy[1]),
                        "type": "seed"
                    })
                    
                # Rounds å¯èƒ½å« rounds, modification_rounds, generations
                rounds = items.get("rounds") or items.get("modification_rounds") or items.get("generations") or []
                
                for round_idx, r in enumerate(rounds):
                    # æ£€æŸ¥é‡Œé¢çš„å­—æ®µ (4a/4b æ··åˆ)
                    # 4a: cross_embedding, mutated_embedding
                    # 4b: child_embedding, selected_embedding, etc.
                    keys_to_check = [
                        "cross_embedding", "mutated_embedding", "child_embedding", "selected_embedding", 
                        "random_mod_embedding", "optimized_embedding"
                    ]
                    
                    for k in keys_to_check:
                         if k in r:
                            emb = np.array(r[k]).reshape(1, -1)
                            xy = pca.transform(emb)[0]
                            # å°è¯•æ‰¾å¯¹åº”çš„æ–‡æœ¬
                            txt_key = k.replace("_embedding", "_result") # e.g. cross_result
                            if txt_key not in r:
                                txt_key = "text" # fallback
                                
                            points.append({
                                "dataset": "Exp4",
                                "id": f"{key}_r{round_idx}_{k.split('_')[0]}",
                                "text": str(r.get(txt_key, r.get("text", "")))[:50],
                                "x": safe_float(xy[0]),
                                "y": safe_float(xy[1]),
                                "type": "evolution"
                            })

        except Exception as e:
            print(f"âŒ Error processing Exp4 {fname}: {e}")
            
    return points

def main():
    pca = load_pca()
    if not pca:
        return

    all_points = []
    all_points.extend(process_exp1(pca))
    all_points.extend(process_exp2(pca))
    all_points.extend(process_exp4(pca))

    print(f"Total points collected: {len(all_points)}")
    
    # Save
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(all_points, f, ensure_ascii=False, separators=(',', ':'))
    
    print(f"ğŸ‰ Saved analysis dataset to {OUTPUT_PATH}")

if __name__ == "__main__":
    main()
