## 一、逐文件：每个文件做什么

### 1) `app/main.py`

- **后端入口**：创建 FastAPI 应用、开 CORS、把各个路由挂载起来。
- 你一般只在这里做：
  - 加/删路由 `include_router(...)`
  - 调整 CORS（开发阶段一般放开）

✅ **不要频繁改动**：改坏了会导致所有接口都不可用。

------

### 2) `app/core/config.py`

- **配置集中管理**：数据文件路径、环境变量（LLM API key / base url 等）。
- 后续你们换成数据库、换存储路径、或者接不同大模型，都从这里拿配置。

✅ 你可以改配置项，但**不要把业务逻辑写进来**。

------

### 3) `app/models/schemas.py`

- **接口数据结构定义**（请求体/响应体/Turn 数据类型）。
- 作用是保证接口返回结构稳定，前端对接不容易炸。

✅ **不建议随便改字段名**（前端已经按这些字段画图了）。

------

### 4) `app/routes/story.py`

- **REST 接口**（前端 axios 调的那一批）：
  - `/api/story/create`：新建 story_id
  - `/api/story/{story_id}`：取整段故事（用于回放）
  - `/api/story/continue`：一次性续写（非流式）

它做的事情是“**编排**”：接收请求 → 调 llm_proxy 生成 → scoring 打分 → storage 存储 → 返回。

✅ 你可以扩展接口，但尽量别破坏现有接口路径/返回结构。

------

### 5) `app/routes/metrics.py`

- **指标/对比相关接口**：
  - `/api/metrics/compare?story_id=...`：返回 points（用于散点对比图）

现在是简化版：直接把 story 的 turns 返回。后续要分组（human vs ai）也可以在这里做。

✅ 这是你后续“加工作量”的好地方（做统计、聚合、对比）。

------

### 6) `app/routes/ws.py`

- **WebSocket 实时推送接口**：
  - `/ws/story/{story_id}`：前端连上后，发 `{user_text, rounds, mode}`
     后端会“边生成边推送”，每生成一句就 `send_json` 一条 turn。

它负责：

- 接收前端消息
- 先推 human turn（并存储）
- 再调用 llm_proxy 的流式生成
- 每条生成结果都打分、存储、推给前端

✅ 前端“酷炫动态加点”主要靠它。
 ✅ **不要随便改 WebSocket 地址和推送 JSON 字段**，否则前端要同步改。

------

### 7) `app/services/storage.py`

- **存取数据**（目前用 `data/stories.json` 当数据库）
- 负责：
  - 创建故事（生成 story_id）
  - 获取故事 turns
  - 追加 turns（并统一分配 turn 序号）

✅ 这是“数据一致性”的核心。
 ✅ **大模型对接不需要动它**。
 （以后换 sqlite/mysql 也主要改它，不动路由和前端。）

------

### 8) `app/services/scoring.py`

- **指标计算**（flow_score、entropy_score）
- 现在是占位版（基于文本长度/字符多样性），方便先跑通全链路。
- 后面你们要上 embedding 相似度/困惑度等，主要改这里。

✅ 你们的“研究价值”基本就在这里升级。
 ✅ **字段名别改**：仍然输出 `flow_score` 和 `entropy_score`。

------

### 9) `app/services/llm_proxy.py`（队友主要要改的地方）

- **大模型接入层**（隔离变化）
- 你们现在 mock：
  - `generate_ai_turns(...)`：REST 一次生成 N 句
  - `stream_ai_turns(...)`：WS 流式生成 N 句

✅ 队友接大模型，**基本只动这个文件**。
 ✅ 你其他文件尽量不动，保证你们对接稳定。

------

### 10) `data/stories.json`

- **数据文件**：保存 story_id -> turns 列表
- 方便你们调试/回放/对比，不用搭数据库。

✅ 别手动乱改格式（会导致读取失败）。

------

## 二、你需要队友给你什么？怎么对接？

你需要队友明确 4 件事（让他们按这个交付）：

### 1) 生成接口的“输入/输出契约”（最重要）

你们约定：`llm_proxy.py` 的两个函数返回的数据结构必须是：

- 每条 turn 至少包含：
  - `author`: `"ai"`（或者 `"human"`，但 AI 生成必须是 ai）
  - `text`: 生成文本字符串

**不要让队友改字段名**，否则 scoring/storage/ws/story 都要跟着改。

------

### 2) 流式 vs 非流式怎么支持

- 如果他们的大模型支持流式（streaming token）：
   他们就把 `stream_ai_turns()` 改成“每生成一句 yield 一次”或“每生成一段 yield 一次”（你前端可实时加点）。
- 如果不支持流式：
   也没关系，他们可以在 `stream_ai_turns()` 里：
  - 一次拿到完整结果
  - 再拆成 N 段/按句号切分
  - `await asyncio.sleep(0.2)` 模拟逐条 yield
     前端依旧能“动态刷新”，展示效果不变。

------

### 3) 他们需要从你这里拿什么上下文？

你要告诉队友：生成时可能需要：

- `story_id`
- 当前上下文 turns（历史对话/故事）

所以你们可以加一个约定：
 在 `llm_proxy.py` 内部如果需要上下文，就去 `storage.get_story_turns(story_id)` 取历史 turns，拼 prompt。

**对接方式建议：**

- 队友在 `llm_proxy.py` 内部自己取上下文（最省事）
- 你不用改 routes 层

------

### 4) 失败时怎么返回（别让系统崩）

要求队友：

- 生成失败要抛异常 or 返回空列表，你这边 WS/REST 会处理
- 最好在 `llm_proxy.py` 内 catch 并返回“可读错误文本”（也能演示）

------

## 三、哪些东西让队友别动？你也别动？

### ✅ 队友尽量只动：

- `app/services/llm_proxy.py`

### ✅ 你可以动（增强系统/指标/展示）：

- `app/services/scoring.py`（升级指标算法）
- `app/routes/metrics.py`（做对比统计、聚合）
- `app/services/storage.py`（未来换 sqlite/mysql）

### ❌ 尽量别动（动了要全链路一起改）：

- `app/routes/ws.py` 的地址：`/ws/story/{story_id}`
- `app/routes/story.py` 的接口路径：`/api/story/create`、`/continue`、`/{id}`
- `schemas.py` 里的字段名：`author/text/flow_score/entropy_score/turn/story_id`

这些是前端已经依赖的“接口契约”。

------

## 四、最清晰的对接清单（你可以直接发给队友）

你把这段直接丢给队友就行：

**你需要他们交付：**

1. 在 `app/services/llm_proxy.py` 实现：
   - `generate_ai_turns(story_id, user_text, rounds, mode) -> List[{"author":"ai","text":...}]`
   - `stream_ai_turns(story_id, user_text, rounds, mode) -> async generator yield {"author":"ai","text":...}`
2. 不改其它文件；不改字段名；不改路由路径。
3. 如需上下文，自行从 storage 读取 turns 拼 prompt。